# -*- coding: utf-8 -*-
"""Text_Extractor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k4k3URPKY04m7GxBm5Bd-GBdgfoQ8VRv

This code is used to extract all of the text from files and put them in excel files.

We create to files that we are going to use later for searching.

One of the dataframes (which we later convert to excel files) contains all the metadata from the files.

The other dataframe contains individual page information for every single page from eveyr single file.
"""

#!pip install tika
#!pip install nltk
#!pip install BeautifulSoup
#!pip install pprint
#!pip install json
#!pip install pyspellchecker
import nltk
import re
import json
import pprint
import pandas as pd
import string
nltk.download('punkt')
nltk.download('stopwords')
from tika import parser
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
from spellchecker import SpellChecker
from google.cloud import storage

client = storage.Client.create_anonymous_client()
bucket_name="rapporteur-pdfs"
bucket = client.bucket(bucket_name=bucket_name, user_project=None)

client = storage.Client.create_anonymous_client()
# you need to set user_project to None for anonymous access
# If not it will attempt to put egress bill on the project you specify,
# and then you need to be authenticated to that project.

n_pdf = 400
cols_metadata = ['pdf_id', 'title', 'author', 'creation', 'last_modified', 'keywords', 'npages']
cols_page = ['pdf_id', 'text', 'tokenize', 'keywords', 'page_num']
df_metadata = pd.DataFrame(index = range(n_pdf), columns = cols_metadata)
df_page = pd.DataFrame(index = range(80000), columns = cols_par)
z = 0
counter = 0
punctuations = ['(',')',';',':','[',']',',', '>', '<']
stop_words = stopwords.words('dutch')
for b in bucket.list_blobs():
  url = b.public_url
  raw = parser.from_file(url, xmlContent=True)
  pdf = raw['content']
  pdf_metadata = raw['metadata']
  n_page = int(raw['metadata']['xmpTPg:NPages'])
  df_metadata.loc[z] = [z, b.name, pdf_metadata.get('Author'), pdf_metadata.get('Creation-Date'), pdf_metadata.get('Last-Modified'), pdf_metadata.get('Keywords'), n_page]
  pdf_by_page = pdf.split('class="page"')
  k = 1
  for page in pdf_by_page:
    page_soup = BeautifulSoup(page, 'html.parser')
    text = page_soup.getText().replace('\n','').replace('-', '').lower()
    tokenized = word_tokenize(text)
    keywords = [word for word in tokenized if not word in stop_words and not word in punctuations]
    df_page.loc[counter] = [z, text, tokenized, keywords, k]
    counter += 1
    k += 1
  z += 1

fields = ['date', 'pdf_names', 'pub_type', 'summary_clean', 'title']
df_scraped = pd.read_excel('cleaned_data_correct_pdf_names.xlsx', usecols=fields)

df_cd = pd.merge(df_metadata.dropna(how='all'), df_scraped, left_on='title', right_on='pdf_names', how='left')

df_cd.to_excel('pdf_metadata.xlsx')

df_page.dropna(how='all').to_excel('pdf_pages.xlsx')